{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader for labeled episodes with embedded sentences\n",
    "\n",
    "Need to have embeddings saved from Model_Pretraining\n",
    "\n",
    "Initiating the class takes some time to load all the embeddings into memory. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for LSTM Data loader\n",
    "def ls_data_loader(path):\n",
    "    # get path to all _emb files\n",
    "    emb_files = glob(path + '/*_emb.bin')\n",
    "    lab_files = glob(path + '/*_lab.bin')\n",
    "    data, labels = [],[]\n",
    "    for e,l in zip(emb_files, lab_files):\n",
    "        data.append(np.loadtxt(e))\n",
    "        labels.append(np.loadtxt(l))\n",
    "    return data, labels\n",
    "\n",
    "class Dataset_seq_ep(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_path):\n",
    "        # self.sent_id = sent_id\n",
    "        self.train_path = train_path\n",
    "        self.data, self.labels = ls_data_loader(train_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return sequence of sentences and labels\n",
    "        seq = torch.Tensor(self.data[index])\n",
    "        labels = torch.Tensor(self.labels[index])\n",
    "        return seq, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\t'''  \n",
    "\tcustom collate_fn as the size of every episode is different and merging sequences (including padding) \n",
    "\tis not supported in default. \n",
    "\t'''\n",
    "\n",
    "\t(xx, yy) = zip(*batch)\n",
    "\tx_lens = [len(x) for x in xx]\n",
    "\ty_lens = [len(y) for y in yy]\n",
    "\n",
    "\txx_pad = pad_sequence(xx, batch_first=True, padding_value=-1)\n",
    "\tyy_pad = pad_sequence(yy, batch_first=True, padding_value=-1)\n",
    "\n",
    "\treturn xx_pad, yy_pad, x_lens, y_lens\n",
    "\n",
    "train_dataset = Dataset_seq_ep('labeled_subs')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataloader and see that batches are padded to seq with max_length per batch and the length of each sequence is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    loop = tqdm(train_dataloader)\n",
    "    for batch in loop:\n",
    "        data, labels, in_len, lab_len = batch\n",
    "        print('Data shape', data.shape)\n",
    "        print('labels shape', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM model\n",
    "\n",
    "adapting from \n",
    "\n",
    "Blog post:\n",
    "Taming LSTMs: Variable-sized mini-batches and why PyTorch is good for your health:\n",
    "https://medium.com/@_willfalcon/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is still a work in progress\n",
    "\n",
    "class PerpLSTM(nn.Module):\n",
    "    def __init__(self, nb_lstm_layers, nb_lstm_units=100, fc_hidden_units=100, embedding_dim=3, batch_size=1):\n",
    "        super(PerpLSTM, self).__init__()\n",
    "        self.vocab = {'<PAD>': -1} # not sure we need this\n",
    "        self.tags = {'<PAD>':-1, 'N': 0, 'Y': 1}\n",
    "\n",
    "        self.nb_lstm_layers = nb_lstm_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_hidden_units = fc_hidden_units\n",
    "\n",
    "        # don't count the padding tag for the classifier output\n",
    "        self.nb_tags = len(self.tags)-1\n",
    "\n",
    "        # when the model is bidirectional we double the output dimension\n",
    "        # self.lstm\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        # build embedding layer first\n",
    "        # nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        # # whenever the embedding sees the padding index it'll make the whole vector zeros\n",
    "        # padding_idx = self.vocab['<PAD>']\n",
    "        # self.word_embedding = nn.Embedding(\n",
    "        #     num_embeddings=nb_vocab_words,\n",
    "        #     embedding_dim=self.embedding_dim,\n",
    "        #     padding_idx=padding_idx\n",
    "        # )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # design LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # output layer which projects back to tag space\n",
    "        self.fc1 = nn.Linear(self.nb_lstm_units, self.fc_hidden_units)\n",
    "        self.fc2 = nn.Linear(self.fc_hidden_units, 1) # change out to 1 for sigmoid activation\n",
    "        # self.relu2 = nn.ReLU()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
    "        hidden_b = torch.randn(self.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
    "\n",
    "        # if self.on_gpu:\n",
    "        #     hidden_a = hidden_a.cuda()\n",
    "        #     hidden_b = hidden_b.cuda()\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # run through ReLu\n",
    "        X = self.relu(X)\n",
    "\n",
    "        # run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "\n",
    "        # undo the packing operation\n",
    "        # X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # ---------------------\n",
    "        # 3. Project to tag space\n",
    "        # Dim transformation: (batch_size, seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_lstm_units)\n",
    "\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = self.fc1(X)\n",
    "        X = self.relu(X)\n",
    "        X = torch.sigmoid(self.fc2(X))\n",
    "\n",
    "        # ---------------------\n",
    "        # 4. Create softmax activations bc we're doing classification\n",
    "        # Dim transformation: (batch_size * seq_len, nb_lstm_units) -> (batch_size, seq_len, nb_tags)\n",
    "        # X = F.log_softmax(X, dim=1)\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        # X = X.view(batch_size, seq_len, self.nb_tags)\n",
    "\n",
    "        Y_hat = X\n",
    "        return Y_hat\n",
    "\n",
    "    def loss(self, Y_hat, Y, X_lengths):\n",
    "        # TRICK 3 ********************************\n",
    "        # before we calculate the negative log likelihood, we need to mask out the activations\n",
    "        # this means we don't want to take into account padded items in the output vector\n",
    "        # simplest way to think about this is to flatten ALL sequences into a REALLY long sequence\n",
    "        # and calculate the loss on that.\n",
    "\n",
    "        # flatten all the labels\n",
    "        Y = Y.view(-1)\n",
    "        print('Y',Y)\n",
    "        abs_y = torch.abs(Y)\n",
    "\n",
    "        # flatten all predictions\n",
    "        Y_hat = Y_hat.view(-1, self.nb_tags)\n",
    "        print('Y hat',Y_hat)\n",
    "        # create a mask by filtering out all tokens that ARE NOT the padding token\n",
    "        tag_pad_token = self.tags['<PAD>']\n",
    "        mask = (Y > tag_pad_token).float()\n",
    "\n",
    "        print('mask', mask)\n",
    "\n",
    "        # count how many tokens we have\n",
    "        # nb_tokens = int(torch.sum(mask).data[0])\n",
    "        nb_tokens = 2\n",
    "        print(Y_hat.shape)\n",
    "        \n",
    "        # pick the values for the label and zero out the rest with the mask\n",
    "\n",
    "        # this part is broken\n",
    "\n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), abs_y] * mask\n",
    "        \n",
    "\n",
    "        # compute cross entropy loss which ignores all <PAD> tokens\n",
    "        ce_loss = -torch.sum(Y_hat) / nb_tokens\n",
    "\n",
    "        return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perp_model = PerpLSTM(\n",
    "    nb_lstm_layers=10, \n",
    "    nb_lstm_units=10, \n",
    "    embedding_dim=30522, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape torch.Size([1, 573, 30522])\n",
      "labels shape torch.Size([1, 573])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([573, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    loop = tqdm(train_dataloader)\n",
    "    for batch in loop:\n",
    "        data, labels, in_len, lab_len = batch\n",
    "        print('Data shape', data.shape)\n",
    "        print('labels shape', labels.shape)\n",
    "        output = perp_model.forward(data, in_len)\n",
    "        print(output.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [02:23<00:00,  3.68s/it, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.41181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:30<00:00,  2.33s/it, loss=0.526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.41219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [02:00<00:00,  3.10s/it, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, loss: 0.41157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:51<00:00,  2.85s/it, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, loss: 0.41155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:34<00:00,  2.43s/it, loss=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss: 0.41122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:33<00:00,  2.39s/it, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, loss: 0.41115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:33<00:00,  2.39s/it, loss=0.709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, loss: 0.41087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:17<00:00,  2.00s/it, loss=0.39] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, loss: 0.41121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:17<00:00,  2.00s/it, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, loss: 0.41141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:20<00:00,  2.06s/it, loss=0.479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, loss: 0.41121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(perp_model.parameters(), lr=0.001) \n",
    "# Using Binary Cross Entropy Loss function since we are using batch size = 1\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    loop = tqdm(train_dataloader)\n",
    "    epoch_total = 0\n",
    "    for batch in loop:\n",
    "        data, labels, in_len, lab_len = batch\n",
    "\n",
    "        outputs = perp_model.forward(data, in_len) #forward pass\n",
    "        optimizer.zero_grad() #calculate the gradient, manually setting to 0\n",
    " \n",
    "        # obtain the loss function\n",
    "        # loss = perp_model.loss(outputs, labels, lab_len)\n",
    "        # m = nn.Sigmoid()\n",
    "        # sig_out = m(outputs)\n",
    "        # print(sig_out.shape)\n",
    "\n",
    "        # loss = criterion(outputs, F.one_hot(labels.view(-1).type(torch.int64)).type(torch.float32))\n",
    "        loss = criterion(outputs.view(1,-1), labels.type(torch.float32))\n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "        \n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        epoch_total += loss.item()\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, epoch_total/len(loop)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1389],\n",
       "        [0.1468],\n",
       "        [0.1474],\n",
       "        [0.1473],\n",
       "        [0.1467],\n",
       "        [0.1463],\n",
       "        [0.1461],\n",
       "        [0.1459],\n",
       "        [0.1457],\n",
       "        [0.1456],\n",
       "        [0.1454],\n",
       "        [0.1454],\n",
       "        [0.1453],\n",
       "        [0.1453],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452],\n",
       "        [0.1452]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perp_model(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del batch, data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "648bdb013ee8df11fcd6d1788aa4da626fcc7525b2ce94a1c13cce96f6951910"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
